{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ad4baf",
   "metadata": {},
   "source": [
    "# Capstone EDA\n",
    "By : Pythonic Minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "711972a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of Contents  \n",
    "# this is develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "478ee34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Overview of the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e99ea",
   "metadata": {},
   "source": [
    "### **a. Business Problems**\n",
    "Home Credit aims to expand financial services to the unbanked population, where traditional credit history is limited or non-existent. To assess creditworthiness, they supplement loan applications with alternative data such as transaction records and telecom behavior.\n",
    "\n",
    "The business objective is to predict the likelihood of loan default, where 0 indicates no default and 1 represents at least one late payment. Accurately predicting defaults will help Home Credit minimize losses and promote financial inclusion while ensuring responsible lending. This project aims to build a predictive model to assess clients' loan repayment ability using available data.\n",
    "\n",
    "### **b. Analytical Methodology**\n",
    "Default risk prediction will be generated using a supervised machine learning classification model. Historical loan application data will be split into two sets, 80% for training and 20% for test. A model will be built based on the trained dataset and test its performance on the test set. The model will analyze factors like applicant demographics, credit history, past loan performance and repayment behavior to predict the likeliness of clients’ default on loans.\n",
    "\n",
    "### **c. Goal**\n",
    "Reducing the likelihood of default: by identifying the main predictors that are positively and negatively correlated with it\n",
    "\n",
    "## **2. Introduction**\n",
    "The notebook begins by exploring the application_train.csv file, which contains key demographic information about the applicants, such as age, income, along with their credit history and other significant documents. Throughout the notebook, extensive data cleaning is performed to ensure the dataset is in optimal shape for analysis. Furthermore, the notebook focuses on identifying the most important predictors that influence the likelihood of defaulting. By carefully removing unnecessary rows and columns, the analysis reduces noise and enhances the model's overall efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b603a919",
   "metadata": {},
   "source": [
    "## 3. Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1449563-cbce-4a83-8cb7-98e5e9f8c381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version\n",
      "---------------------------------- --------------------\n",
      "alabaster                          0.7.12\n",
      "anaconda-client                    1.9.0\n",
      "anaconda-navigator                 2.1.1\n",
      "anaconda-project                   0.10.1\n",
      "anyio                              2.2.0\n",
      "appdirs                            1.4.4\n",
      "argh                               0.26.2\n",
      "argon2-cffi                        20.1.0\n",
      "arrow                              0.13.1\n",
      "asn1crypto                         1.4.0\n",
      "astroid                            2.6.6\n",
      "astropy                            4.3.1\n",
      "asttokens                          3.0.0\n",
      "async-generator                    1.10\n",
      "atomicwrites                       1.4.0\n",
      "attrs                              21.2.0\n",
      "autopep8                           1.5.7\n",
      "Babel                              2.9.1\n",
      "backcall                           0.2.0\n",
      "backports.functools-lru-cache      1.6.4\n",
      "backports.shutil-get-terminal-size 1.0.0\n",
      "backports.tempfile                 1.0\n",
      "backports.weakref                  1.0.post1\n",
      "bcrypt                             3.2.0\n",
      "beautifulsoup4                     4.10.0\n",
      "binaryornot                        0.4.4\n",
      "bitarray                           2.3.0\n",
      "bkcharts                           0.2\n",
      "black                              19.10b0\n",
      "bleach                             4.0.0\n",
      "bokeh                              2.4.1\n",
      "boto                               2.49.0\n",
      "Bottleneck                         1.4.2\n",
      "brotlipy                           0.7.0\n",
      "cached-property                    1.5.2\n",
      "certifi                            2021.10.8\n",
      "cffi                               1.14.6\n",
      "chardet                            4.0.0\n",
      "charset-normalizer                 2.0.4\n",
      "click                              8.0.3\n",
      "cloudpickle                        2.0.0\n",
      "clyent                             1.2.2\n",
      "colorama                           0.4.4\n",
      "comtypes                           1.1.10\n",
      "conda                              4.10.3\n",
      "conda-build                        3.21.6\n",
      "conda-content-trust                0+unknown\n",
      "conda-pack                         0.6.0\n",
      "conda-package-handling             1.7.3\n",
      "conda-repo-cli                     1.0.4\n",
      "conda-token                        0.3.0\n",
      "conda-verify                       3.4.2\n",
      "contextlib2                        0.6.0.post1\n",
      "cookiecutter                       1.7.2\n",
      "cryptography                       3.4.8\n",
      "cycler                             0.10.0\n",
      "Cython                             0.29.24\n",
      "cytoolz                            0.11.0\n",
      "daal4py                            2021.3.0\n",
      "dask                               2021.10.0\n",
      "debugpy                            1.4.1\n",
      "decorator                          5.1.0\n",
      "defusedxml                         0.7.1\n",
      "diff-match-patch                   20200713\n",
      "distributed                        2021.10.0\n",
      "docutils                           0.17.1\n",
      "entrypoints                        0.3\n",
      "et-xmlfile                         1.1.0\n",
      "exceptiongroup                     1.2.2\n",
      "executing                          2.2.0\n",
      "fastcache                          1.1.0\n",
      "filelock                           3.3.1\n",
      "flake8                             3.9.2\n",
      "Flask                              1.1.2\n",
      "fonttools                          4.25.0\n",
      "fsspec                             2021.10.1\n",
      "future                             0.18.2\n",
      "gevent                             21.8.0\n",
      "glob2                              0.7\n",
      "greenlet                           1.1.1\n",
      "h5py                               3.2.1\n",
      "HeapDict                           1.0.1\n",
      "html5lib                           1.1\n",
      "idna                               3.2\n",
      "imagecodecs                        2021.8.26\n",
      "imageio                            2.9.0\n",
      "imagesize                          1.2.0\n",
      "imbalanced-learn                   0.8.0\n",
      "imblearn                           0.0\n",
      "import-ipynb                       0.2\n",
      "importlib-metadata                 4.8.1\n",
      "inflection                         0.5.1\n",
      "iniconfig                          1.1.1\n",
      "intervaltree                       3.1.0\n",
      "ipykernel                          6.4.1\n",
      "ipython                            8.18.1\n",
      "ipython-genutils                   0.2.0\n",
      "ipython-sql                        0.5.0\n",
      "ipywidgets                         7.6.5\n",
      "isort                              5.9.3\n",
      "itsdangerous                       2.0.1\n",
      "Janitor                            0.1.1\n",
      "jdcal                              1.4.1\n",
      "jedi                               0.18.0\n",
      "Jinja2                             2.11.3\n",
      "jinja2-time                        0.2.0\n",
      "joblib                             1.4.2\n",
      "json5                              0.9.6\n",
      "jsonschema                         3.2.0\n",
      "jupyter                            1.0.0\n",
      "jupyter-client                     6.1.12\n",
      "jupyter-console                    6.4.0\n",
      "jupyter-core                       4.8.1\n",
      "jupyter-server                     1.4.1\n",
      "jupyterlab                         3.2.1\n",
      "jupyterlab-pygments                0.1.2\n",
      "jupyterlab-server                  2.8.2\n",
      "jupyterlab-widgets                 1.0.0\n",
      "keyring                            23.1.0\n",
      "kiwisolver                         1.3.1\n",
      "lazy-object-proxy                  1.6.0\n",
      "libarchive-c                       2.9\n",
      "llvmlite                           0.37.0\n",
      "locket                             0.2.1\n",
      "lxml                               4.6.3\n",
      "MarkupSafe                         1.1.1\n",
      "matplotlib                         3.4.3\n",
      "matplotlib-inline                  0.1.2\n",
      "mccabe                             0.6.1\n",
      "menuinst                           1.4.18\n",
      "mistune                            0.8.4\n",
      "mkl-fft                            1.3.1\n",
      "mkl-random                         1.2.2\n",
      "mkl-service                        2.4.0\n",
      "mock                               4.0.3\n",
      "more-itertools                     8.10.0\n",
      "mpmath                             1.2.1\n",
      "msgpack                            1.0.2\n",
      "multipledispatch                   0.6.0\n",
      "munkres                            1.1.4\n",
      "mypy-extensions                    0.4.3\n",
      "natsort                            8.4.0\n",
      "navigator-updater                  0.2.1\n",
      "nbclassic                          0.2.6\n",
      "nbclient                           0.5.3\n",
      "nbconvert                          6.1.0\n",
      "nbformat                           5.1.3\n",
      "nest-asyncio                       1.5.1\n",
      "networkx                           2.6.3\n",
      "nltk                               3.6.5\n",
      "nose                               1.3.7\n",
      "notebook                           6.4.5\n",
      "numba                              0.54.1\n",
      "numexpr                            2.10.2\n",
      "numpy                              1.26.4\n",
      "numpydoc                           1.1.0\n",
      "olefile                            0.46\n",
      "openpyxl                           3.0.9\n",
      "packaging                          24.2\n",
      "pandas                             2.2.3\n",
      "pandas-flavor                      0.6.0\n",
      "pandocfilters                      1.4.3\n",
      "paramiko                           2.7.2\n",
      "parso                              0.8.2\n",
      "partd                              1.2.0\n",
      "path                               16.0.0\n",
      "pathlib2                           2.3.6\n",
      "pathspec                           0.7.0\n",
      "patsy                              1.0.1\n",
      "pep8                               1.7.1\n",
      "pexpect                            4.8.0\n",
      "pickleshare                        0.7.5\n",
      "Pillow                             8.4.0\n",
      "pip                                21.2.4\n",
      "pkginfo                            1.7.1\n",
      "pluggy                             0.13.1\n",
      "ply                                3.11\n",
      "poyo                               0.5.0\n",
      "prettytable                        3.14.0\n",
      "prometheus-client                  0.11.0\n",
      "prompt_toolkit                     3.0.50\n",
      "psutil                             5.8.0\n",
      "ptyprocess                         0.7.0\n",
      "pure_eval                          0.2.3\n",
      "py                                 1.10.0\n",
      "pycodestyle                        2.7.0\n",
      "pycosat                            0.6.3\n",
      "pycparser                          2.20\n",
      "pycurl                             7.44.1\n",
      "pydocstyle                         6.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f99e6ae-470d-4dba-bacd-68ff1d38ac7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyerfa                             2.0.0\n",
      "pyflakes                           2.3.1\n",
      "Pygments                           2.10.0\n",
      "pyjanitor                          0.30.0\n",
      "PyJWT                              2.1.0\n",
      "pylint                             2.9.6\n",
      "pyls-spyder                        0.4.0\n",
      "PyNaCl                             1.4.0\n",
      "pyodbc                             4.0.0-unsupported\n",
      "pyOpenSSL                          21.0.0\n",
      "pyparsing                          3.0.4\n",
      "pyreadline                         2.1\n",
      "pyrsistent                         0.18.0\n",
      "PySocks                            1.7.1\n",
      "pytest                             6.2.4\n",
      "python-dateutil                    2.8.2\n",
      "python-lsp-black                   1.0.0\n",
      "python-lsp-jsonrpc                 1.0.0\n",
      "python-lsp-server                  1.2.4\n",
      "python-slugify                     5.0.2\n",
      "pytz                               2021.3\n",
      "PyWavelets                         1.1.1\n",
      "pywin32                            228\n",
      "pywin32-ctypes                     0.2.0\n",
      "pywinpty                           0.5.7\n",
      "PyYAML                             6.0\n",
      "pyzmq                              22.2.1\n",
      "QDarkStyle                         3.0.2\n",
      "qstylizer                          0.1.10\n",
      "QtAwesome                          1.0.2\n",
      "qtconsole                          5.1.1\n",
      "QtPy                               1.10.0\n",
      "regex                              2021.8.3\n",
      "requests                           2.26.0\n",
      "rope                               0.19.0\n",
      "Rtree                              0.9.7\n",
      "ruamel-yaml-conda                  0.15.100\n",
      "scikit-image                       0.18.3\n",
      "scikit-learn                       0.24.2\n",
      "scikit-learn-intelex               2021.20210714.120553\n",
      "scipy                              1.13.1\n",
      "seaborn                            0.11.2\n",
      "Send2Trash                         1.8.0\n",
      "setuptools                         58.0.4\n",
      "simplegeneric                      0.8.1\n",
      "singledispatch                     3.7.0\n",
      "sip                                4.19.13\n",
      "six                                1.16.0\n",
      "sniffio                            1.2.0\n",
      "snowballstemmer                    2.1.0\n",
      "sortedcollections                  2.1.0\n",
      "sortedcontainers                   2.4.0\n",
      "soupsieve                          2.2.1\n",
      "Sphinx                             4.2.0\n",
      "sphinxcontrib-applehelp            1.0.2\n",
      "sphinxcontrib-devhelp              1.0.2\n",
      "sphinxcontrib-htmlhelp             2.0.0\n",
      "sphinxcontrib-jsmath               1.0.1\n",
      "sphinxcontrib-qthelp               1.0.3\n",
      "sphinxcontrib-serializinghtml      1.1.5\n",
      "sphinxcontrib-websupport           1.2.4\n",
      "spyder                             5.1.5\n",
      "spyder-kernels                     2.1.3\n",
      "SQLAlchemy                         2.0.38\n",
      "sqlparse                           0.5.3\n",
      "stack-data                         0.6.3\n",
      "statsmodels                        0.14.4\n",
      "sympy                              1.9\n",
      "tables                             3.6.1\n",
      "TBB                                0.2\n",
      "tblib                              1.7.0\n",
      "terminado                          0.9.4\n",
      "testpath                           0.5.0\n",
      "text-unidecode                     1.3\n",
      "textdistance                       4.2.1\n",
      "threadpoolctl                      3.5.0\n",
      "three-merge                        0.1.1\n",
      "tifffile                           2021.7.2\n",
      "tinycss                            0.4\n",
      "toml                               0.10.2\n",
      "toolz                              0.11.1\n",
      "tornado                            6.1\n",
      "tqdm                               4.62.3\n",
      "traitlets                          5.1.0\n",
      "typed-ast                          1.4.3\n",
      "typing_extensions                  4.12.2\n",
      "tzdata                             2025.1\n",
      "ujson                              4.0.2\n",
      "unicodecsv                         0.14.1\n",
      "Unidecode                          1.2.0\n",
      "urllib3                            1.26.7\n",
      "watchdog                           2.1.3\n",
      "wcwidth                            0.2.5\n",
      "webencodings                       0.5.1\n",
      "Werkzeug                           2.0.2\n",
      "wheel                              0.37.0\n",
      "whichcraft                         0.6.1\n",
      "widgetsnbextension                 3.5.1\n",
      "win-inet-pton                      1.1.0\n",
      "win-unicode-console                0.5\n",
      "wincertstore                       0.2\n",
      "wrapt                              1.12.1\n",
      "wsgi-oauth2                        0.2.2\n",
      "xarray                             2023.12.0\n",
      "xlrd                               2.0.1\n",
      "XlsxWriter                         3.0.1\n",
      "xlwings                            0.24.9\n",
      "xlwt                               1.3.0\n",
      "xmltodict                          0.12.0\n",
      "yapf                               0.31.0\n",
      "zict                               2.0.0\n",
      "zipp                               3.6.0\n",
      "zope.event                         4.5.0\n",
      "zope.interface                     5.4.0\n",
      "C:\\Users\\91744\\anaconda3\\python.exe\n",
      "C:\\python311\\python.exe\n",
      "C:\\Users\\91744\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\n"
     ]
    }
   ],
   "source": [
    "!where python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4753618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420eb27",
   "metadata": {},
   "source": [
    "## 4. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "342c5e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# define the function for reducing memory usage when importing data\n",
    "def reduce_memory_usage(df):\n",
    "  \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58fdf1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the csv file \n",
    "application_train_df = pd.read_csv(\"application_train.csv\")\n",
    "bureau_df = pd.read_csv(\"bureau.csv\")\n",
    "# bureau_balance_df = pd.read_csv(\"bureau_balance.csv\")\n",
    "# credit_card_balance_df = pd.read_csv(\"credit_card_balance.csv\")\n",
    "# installments_payments_df = pd.read_csv(\"installments_payments.csv\")\n",
    "# pos_cash_balance_df =  pd.read_csv(\"POS_CASH_balance.csv\")\n",
    "# previous_application_df = pd.read_csv(\"previous_application.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e1a4978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 286.23 MB\n",
      "Memory usage after optimization is: 92.38 MB\n",
      "Decreased by 67.7%\n",
      "Memory usage of dataframe is 222.62 MB\n",
      "Memory usage after optimization is: 112.95 MB\n",
      "Decreased by 49.3%\n"
     ]
    }
   ],
   "source": [
    "# Apply the reduce memory usage function to each DataFrame\n",
    "application_train_df = reduce_memory_usage(application_train_df)\n",
    "bureau_df = reduce_memory_usage(bureau_df)\n",
    "# bureau_balance_df = reduce_memory_usage(bureau_balance_df)\n",
    "# credit_card_balance_df = reduce_memory_usage(credit_card_balance_df)\n",
    "# installments_payments_df = reduce_memory_usage(installments_payments_df)\n",
    "# POS_CASH_balance_df = reduce_memory_usage(pos_cash_balance_df)\n",
    "# previous_application_df = reduce_memory_usage(previous_application_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f056db",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Data Exploration\n",
    "In this section, we will understand different features of the all the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f796a7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>...</th>\n",
       "      <th>FLAG_DOCUMENT_18</th>\n",
       "      <th>FLAG_DOCUMENT_19</th>\n",
       "      <th>FLAG_DOCUMENT_20</th>\n",
       "      <th>FLAG_DOCUMENT_21</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>307511.000000</td>\n",
       "      <td>307511.000000</td>\n",
       "      <td>307511.000000</td>\n",
       "      <td>3.075110e+05</td>\n",
       "      <td>3.075110e+05</td>\n",
       "      <td>307499.000000</td>\n",
       "      <td>3.072330e+05</td>\n",
       "      <td>307511.000000</td>\n",
       "      <td>307511.000000</td>\n",
       "      <td>307511.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>307511.000000</td>\n",
       "      <td>307511.000000</td>\n",
       "      <td>307511.000000</td>\n",
       "      <td>307511.000000</td>\n",
       "      <td>265992.000000</td>\n",
       "      <td>265992.000000</td>\n",
       "      <td>265992.000000</td>\n",
       "      <td>265992.0</td>\n",
       "      <td>265992.0</td>\n",
       "      <td>265992.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>278180.518577</td>\n",
       "      <td>0.080729</td>\n",
       "      <td>0.417052</td>\n",
       "      <td>1.687979e+05</td>\n",
       "      <td>5.990259e+05</td>\n",
       "      <td>27108.572266</td>\n",
       "      <td>5.383961e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-16036.995067</td>\n",
       "      <td>63815.045904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>102790.175348</td>\n",
       "      <td>0.272419</td>\n",
       "      <td>0.722121</td>\n",
       "      <td>2.371759e+05</td>\n",
       "      <td>4.024795e+05</td>\n",
       "      <td>14493.233398</td>\n",
       "      <td>3.695427e+05</td>\n",
       "      <td>0.013824</td>\n",
       "      <td>4363.988632</td>\n",
       "      <td>141275.766519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089798</td>\n",
       "      <td>0.024387</td>\n",
       "      <td>0.022518</td>\n",
       "      <td>0.018299</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.110718</td>\n",
       "      <td>0.204712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>100002.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.565000e+04</td>\n",
       "      <td>4.500000e+04</td>\n",
       "      <td>1615.500000</td>\n",
       "      <td>4.050000e+04</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>-25229.000000</td>\n",
       "      <td>-17912.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>189145.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.125000e+05</td>\n",
       "      <td>2.700000e+05</td>\n",
       "      <td>16524.000000</td>\n",
       "      <td>2.385000e+05</td>\n",
       "      <td>0.010010</td>\n",
       "      <td>-19682.000000</td>\n",
       "      <td>-2760.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>278202.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.471500e+05</td>\n",
       "      <td>5.135310e+05</td>\n",
       "      <td>24903.000000</td>\n",
       "      <td>4.500000e+05</td>\n",
       "      <td>0.018845</td>\n",
       "      <td>-15750.000000</td>\n",
       "      <td>-1213.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>367142.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.025000e+05</td>\n",
       "      <td>8.086500e+05</td>\n",
       "      <td>34596.000000</td>\n",
       "      <td>6.795000e+05</td>\n",
       "      <td>0.028656</td>\n",
       "      <td>-12413.000000</td>\n",
       "      <td>-289.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>456255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.170000e+08</td>\n",
       "      <td>4.050000e+06</td>\n",
       "      <td>258025.500000</td>\n",
       "      <td>4.050000e+06</td>\n",
       "      <td>0.072510</td>\n",
       "      <td>-7489.000000</td>\n",
       "      <td>365243.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          SK_ID_CURR         TARGET   CNT_CHILDREN  AMT_INCOME_TOTAL  \\\n",
       "count  307511.000000  307511.000000  307511.000000      3.075110e+05   \n",
       "mean   278180.518577       0.080729       0.417052      1.687979e+05   \n",
       "std    102790.175348       0.272419       0.722121      2.371759e+05   \n",
       "min    100002.000000       0.000000       0.000000      2.565000e+04   \n",
       "25%    189145.500000       0.000000       0.000000      1.125000e+05   \n",
       "50%    278202.000000       0.000000       0.000000      1.471500e+05   \n",
       "75%    367142.500000       0.000000       1.000000      2.025000e+05   \n",
       "max    456255.000000       1.000000      19.000000      1.170000e+08   \n",
       "\n",
       "         AMT_CREDIT    AMT_ANNUITY  AMT_GOODS_PRICE  \\\n",
       "count  3.075110e+05  307499.000000     3.072330e+05   \n",
       "mean   5.990259e+05   27108.572266     5.383961e+05   \n",
       "std    4.024795e+05   14493.233398     3.695427e+05   \n",
       "min    4.500000e+04    1615.500000     4.050000e+04   \n",
       "25%    2.700000e+05   16524.000000     2.385000e+05   \n",
       "50%    5.135310e+05   24903.000000     4.500000e+05   \n",
       "75%    8.086500e+05   34596.000000     6.795000e+05   \n",
       "max    4.050000e+06  258025.500000     4.050000e+06   \n",
       "\n",
       "       REGION_POPULATION_RELATIVE     DAYS_BIRTH  DAYS_EMPLOYED  ...  \\\n",
       "count               307511.000000  307511.000000  307511.000000  ...   \n",
       "mean                     0.000000  -16036.995067   63815.045904  ...   \n",
       "std                      0.013824    4363.988632  141275.766519  ...   \n",
       "min                      0.000290  -25229.000000  -17912.000000  ...   \n",
       "25%                      0.010010  -19682.000000   -2760.000000  ...   \n",
       "50%                      0.018845  -15750.000000   -1213.000000  ...   \n",
       "75%                      0.028656  -12413.000000    -289.000000  ...   \n",
       "max                      0.072510   -7489.000000  365243.000000  ...   \n",
       "\n",
       "       FLAG_DOCUMENT_18  FLAG_DOCUMENT_19  FLAG_DOCUMENT_20  FLAG_DOCUMENT_21  \\\n",
       "count     307511.000000     307511.000000     307511.000000     307511.000000   \n",
       "mean           0.008130          0.000595          0.000507          0.000335   \n",
       "std            0.089798          0.024387          0.022518          0.018299   \n",
       "min            0.000000          0.000000          0.000000          0.000000   \n",
       "25%            0.000000          0.000000          0.000000          0.000000   \n",
       "50%            0.000000          0.000000          0.000000          0.000000   \n",
       "75%            0.000000          0.000000          0.000000          0.000000   \n",
       "max            1.000000          1.000000          1.000000          1.000000   \n",
       "\n",
       "       AMT_REQ_CREDIT_BUREAU_HOUR  AMT_REQ_CREDIT_BUREAU_DAY  \\\n",
       "count               265992.000000              265992.000000   \n",
       "mean                     0.000000                   0.000000   \n",
       "std                      0.083984                   0.110718   \n",
       "min                      0.000000                   0.000000   \n",
       "25%                      0.000000                   0.000000   \n",
       "50%                      0.000000                   0.000000   \n",
       "75%                      0.000000                   0.000000   \n",
       "max                      4.000000                   9.000000   \n",
       "\n",
       "       AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  \\\n",
       "count               265992.000000                   265992.0   \n",
       "mean                     0.000000                        NaN   \n",
       "std                      0.204712                        0.0   \n",
       "min                      0.000000                        0.0   \n",
       "25%                      0.000000                        0.0   \n",
       "50%                      0.000000                        0.0   \n",
       "75%                      0.000000                        0.0   \n",
       "max                      8.000000                       27.0   \n",
       "\n",
       "       AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  \n",
       "count                   265992.0                    265992.0  \n",
       "mean                         NaN                         NaN  \n",
       "std                          NaN                         0.0  \n",
       "min                          0.0                         0.0  \n",
       "25%                          0.0                         0.0  \n",
       "50%                          0.0                         1.0  \n",
       "75%                          0.0                         3.0  \n",
       "max                        261.0                        25.0  \n",
       "\n",
       "[8 rows x 106 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 122)\n"
     ]
    }
   ],
   "source": [
    "#getting a summary statistics and shape of the application dataset\n",
    "summary = application_train_df.describe()\n",
    "display(summary)\n",
    "print(application_train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f06f3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>SK_ID_BUREAU</th>\n",
       "      <th>DAYS_CREDIT</th>\n",
       "      <th>CREDIT_DAY_OVERDUE</th>\n",
       "      <th>DAYS_CREDIT_ENDDATE</th>\n",
       "      <th>DAYS_ENDDATE_FACT</th>\n",
       "      <th>AMT_CREDIT_MAX_OVERDUE</th>\n",
       "      <th>CNT_CREDIT_PROLONG</th>\n",
       "      <th>AMT_CREDIT_SUM</th>\n",
       "      <th>AMT_CREDIT_SUM_DEBT</th>\n",
       "      <th>AMT_CREDIT_SUM_LIMIT</th>\n",
       "      <th>AMT_CREDIT_SUM_OVERDUE</th>\n",
       "      <th>DAYS_CREDIT_UPDATE</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.716428e+06</td>\n",
       "      <td>1.716428e+06</td>\n",
       "      <td>1.716428e+06</td>\n",
       "      <td>1.716428e+06</td>\n",
       "      <td>1610875.0</td>\n",
       "      <td>1082775.0</td>\n",
       "      <td>5.919400e+05</td>\n",
       "      <td>1.716428e+06</td>\n",
       "      <td>1.716415e+06</td>\n",
       "      <td>1.458759e+06</td>\n",
       "      <td>1.124648e+06</td>\n",
       "      <td>1.716428e+06</td>\n",
       "      <td>1.716428e+06</td>\n",
       "      <td>4.896370e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.782149e+05</td>\n",
       "      <td>5.924434e+06</td>\n",
       "      <td>-1.142108e+03</td>\n",
       "      <td>8.181666e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.825417e+03</td>\n",
       "      <td>6.410406e-03</td>\n",
       "      <td>3.549946e+05</td>\n",
       "      <td>1.370851e+05</td>\n",
       "      <td>6.229514e+03</td>\n",
       "      <td>3.791277e+01</td>\n",
       "      <td>-5.937483e+02</td>\n",
       "      <td>1.571276e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.029386e+05</td>\n",
       "      <td>5.322657e+05</td>\n",
       "      <td>7.951649e+02</td>\n",
       "      <td>3.654443e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.059873e+05</td>\n",
       "      <td>9.622391e-02</td>\n",
       "      <td>1.150277e+06</td>\n",
       "      <td>6.790749e+05</td>\n",
       "      <td>4.489666e+04</td>\n",
       "      <td>5.937519e+03</td>\n",
       "      <td>7.207473e+02</td>\n",
       "      <td>3.256556e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000010e+05</td>\n",
       "      <td>5.000000e+06</td>\n",
       "      <td>-2.922000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-42048.0</td>\n",
       "      <td>-42016.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-4.705600e+06</td>\n",
       "      <td>-5.864061e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-4.194700e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.888668e+05</td>\n",
       "      <td>5.463954e+06</td>\n",
       "      <td>-1.666000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1138.0</td>\n",
       "      <td>-1489.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.130000e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-9.080000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.780550e+05</td>\n",
       "      <td>5.926304e+06</td>\n",
       "      <td>-9.870000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-330.0</td>\n",
       "      <td>-897.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.255185e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-3.950000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.674260e+05</td>\n",
       "      <td>6.385681e+06</td>\n",
       "      <td>-4.740000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>474.0</td>\n",
       "      <td>-425.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.150000e+05</td>\n",
       "      <td>4.015350e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-3.300000e+01</td>\n",
       "      <td>1.350000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.562550e+05</td>\n",
       "      <td>6.843457e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.792000e+03</td>\n",
       "      <td>31200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.159872e+08</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>5.850000e+08</td>\n",
       "      <td>1.701000e+08</td>\n",
       "      <td>4.705600e+06</td>\n",
       "      <td>3.756681e+06</td>\n",
       "      <td>3.720000e+02</td>\n",
       "      <td>1.184534e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SK_ID_CURR  SK_ID_BUREAU   DAYS_CREDIT  CREDIT_DAY_OVERDUE  \\\n",
       "count  1.716428e+06  1.716428e+06  1.716428e+06        1.716428e+06   \n",
       "mean   2.782149e+05  5.924434e+06 -1.142108e+03        8.181666e-01   \n",
       "std    1.029386e+05  5.322657e+05  7.951649e+02        3.654443e+01   \n",
       "min    1.000010e+05  5.000000e+06 -2.922000e+03        0.000000e+00   \n",
       "25%    1.888668e+05  5.463954e+06 -1.666000e+03        0.000000e+00   \n",
       "50%    2.780550e+05  5.926304e+06 -9.870000e+02        0.000000e+00   \n",
       "75%    3.674260e+05  6.385681e+06 -4.740000e+02        0.000000e+00   \n",
       "max    4.562550e+05  6.843457e+06  0.000000e+00        2.792000e+03   \n",
       "\n",
       "       DAYS_CREDIT_ENDDATE  DAYS_ENDDATE_FACT  AMT_CREDIT_MAX_OVERDUE  \\\n",
       "count            1610875.0          1082775.0            5.919400e+05   \n",
       "mean                   NaN                NaN            3.825417e+03   \n",
       "std                    NaN                NaN            2.059873e+05   \n",
       "min               -42048.0           -42016.0            0.000000e+00   \n",
       "25%                -1138.0            -1489.0            0.000000e+00   \n",
       "50%                 -330.0             -897.0            0.000000e+00   \n",
       "75%                  474.0             -425.0            0.000000e+00   \n",
       "max                31200.0                0.0            1.159872e+08   \n",
       "\n",
       "       CNT_CREDIT_PROLONG  AMT_CREDIT_SUM  AMT_CREDIT_SUM_DEBT  \\\n",
       "count        1.716428e+06    1.716415e+06         1.458759e+06   \n",
       "mean         6.410406e-03    3.549946e+05         1.370851e+05   \n",
       "std          9.622391e-02    1.150277e+06         6.790749e+05   \n",
       "min          0.000000e+00    0.000000e+00        -4.705600e+06   \n",
       "25%          0.000000e+00    5.130000e+04         0.000000e+00   \n",
       "50%          0.000000e+00    1.255185e+05         0.000000e+00   \n",
       "75%          0.000000e+00    3.150000e+05         4.015350e+04   \n",
       "max          9.000000e+00    5.850000e+08         1.701000e+08   \n",
       "\n",
       "       AMT_CREDIT_SUM_LIMIT  AMT_CREDIT_SUM_OVERDUE  DAYS_CREDIT_UPDATE  \\\n",
       "count          1.124648e+06            1.716428e+06        1.716428e+06   \n",
       "mean           6.229514e+03            3.791277e+01       -5.937483e+02   \n",
       "std            4.489666e+04            5.937519e+03        7.207473e+02   \n",
       "min           -5.864061e+05            0.000000e+00       -4.194700e+04   \n",
       "25%            0.000000e+00            0.000000e+00       -9.080000e+02   \n",
       "50%            0.000000e+00            0.000000e+00       -3.950000e+02   \n",
       "75%            0.000000e+00            0.000000e+00       -3.300000e+01   \n",
       "max            4.705600e+06            3.756681e+06        3.720000e+02   \n",
       "\n",
       "        AMT_ANNUITY  \n",
       "count  4.896370e+05  \n",
       "mean   1.571276e+04  \n",
       "std    3.256556e+05  \n",
       "min    0.000000e+00  \n",
       "25%    0.000000e+00  \n",
       "50%    0.000000e+00  \n",
       "75%    1.350000e+04  \n",
       "max    1.184534e+08  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1716428, 17)\n"
     ]
    }
   ],
   "source": [
    "#getting a summary statistics and shape of the bureau_df dataset\n",
    "summary = bureau_df.describe()\n",
    "display(summary)\n",
    "print(bureau_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27e38637",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bureau_balance_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#getting a summary statistics and shape of the bureau_balance_df dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mbureau_balance_df\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[0;32m      3\u001b[0m display(summary)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(bureau_balance_df\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bureau_balance_df' is not defined"
     ]
    }
   ],
   "source": [
    "# #getting a summary statistics and shape of the bureau_balance_df dataset\n",
    "# summary = bureau_balance_df.describe()\n",
    "# display(summary)\n",
    "# print(bureau_balance_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f64e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #getting a summary statistics and shape of the credit_card_balance_df dataset\n",
    "# summary = credit_card_balance_df.describe()\n",
    "# display(summary)\n",
    "# print(credit_card_balance_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c7ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #getting a summary statistics and shape of the installments_payments_df dataset\n",
    "# summary = installments_payments_df.describe()\n",
    "# display(summary)\n",
    "# print(installments_payments_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cedcf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #getting a summary statistics and shape of the POS_CASH_balance_df dataset\n",
    "# summary = POS_CASH_balance_df.describe()\n",
    "# display(summary)\n",
    "# print(POS_CASH_balance_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a84ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #getting a summary statistics and shape of the previous_application_df dataset\n",
    "# summary = previous_application_df.describe()\n",
    "# display(summary)\n",
    "# print(previous_application_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab882922",
   "metadata": {},
   "outputs": [],
   "source": [
    "### a. Exploring Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e278fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = application_train_df['TARGET'])\n",
    "plt.title('Distribution of the Target Variable')\n",
    "plt.xlabel('Non Default = [0], Default = [1]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b00889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining Target variable proportion\n",
    "target_prop = round(application_train_df.value_counts(subset='TARGET', normalize=True),2)\n",
    "print(target_prop)\n",
    "\n",
    "print(f'The proportion of Non Defaulters [0] is {target_prop[0]}')\n",
    "print(f'The proportion of Defaulters [1] is {target_prop[1]} ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10e738c",
   "metadata": {},
   "source": [
    "Data Description:\n",
    "It appears we have a higher proportion of clients not having payment difficulties on loans compared to clients that have payment difficulties.\n",
    "The above countplot illustrates this with a proportion of 92% for non default compared to 8% for default.\n",
    "\n",
    "Additionally we have roughly 300,000 rows of data on the Train set with 122 columns\n",
    "Whereas we have 48,000 rows of data on the Test set with 121 columns (as it excludes the Target variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ddb155",
   "metadata": {},
   "outputs": [],
   "source": [
    "### b. Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb55deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution on numerical columns\n",
    "# Select numerical columns\n",
    "num_cols = application_train_df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Define batch size for better visualization\n",
    "batch_size = 6  \n",
    "num_batches = int(np.ceil(len(num_cols) / batch_size))\n",
    "\n",
    "# Plot in smaller groups\n",
    "for i in range(num_batches):\n",
    "    batch_cols = num_cols[i * batch_size:(i + 1) * batch_size]\n",
    "    application_train_df[batch_cols].hist(figsize=(15, 10), bins=30)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdd338f",
   "metadata": {},
   "source": [
    "Since the following features have low variations, we can remove them from our analysis:\n",
    "\n",
    "1. FLAG DOCUMENT 13\n",
    "2. FLAG DOCUMENT 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c236f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop FLAG DOCUMENT 13 and 16\n",
    "application_train_df = application_train_df.drop(columns=['FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_16'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692afc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution on categorical columns\n",
    "# Select categorical columns\n",
    "cat_cols = application_train_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define batch size for better visualization\n",
    "batch_size = 1\n",
    "num_batches = int(np.ceil(len(cat_cols) / batch_size))\n",
    "\n",
    "# Plot in smaller groups\n",
    "for i in range(num_batches):\n",
    "    batch_cols = cat_cols[i * batch_size:(i + 1) * batch_size]\n",
    "    \n",
    "    # Plot each categorical column\n",
    "    plt.figure(figsize=(30, 15))\n",
    "    for j, col in enumerate(batch_cols, 1):\n",
    "        plt.subplot(2, 3, j)  # Adjust rows and columns based on your batch size\n",
    "        sns.countplot(data=application_train_df, x=col)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xticks(rotation=90)  # Rotate x-axis labels vertically\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f2ffb",
   "metadata": {},
   "source": [
    "Most of applicants apply for cash loans and the largest type of housing is house/apartment. Interestingly, the number of people within business entity type 3 applying for loans amounts to nearly 70,000, far more than other type of organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcdc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plot for CODE_GENDER vs TARGET\n",
    "sns.countplot(x='CODE_GENDER', hue='TARGET', data=application_train_df)\n",
    "plt.title('CODE_GENDER vs TARGET')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0874fd63",
   "metadata": {},
   "source": [
    "Interestingly, male has a higher proportion of defaulting on loan than women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a416d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create a box plot with 'TARGET' as hue to separate the data\n",
    "sns.boxplot(data=application_train_df, x='NAME_CONTRACT_TYPE', y='AMT_CREDIT', hue='TARGET', palette='Set2')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Bivariate Analysis: NAME_CONTRACT_TYPE vs AMT_CREDIT by TARGET')\n",
    "plt.xlabel('NAME_CONTRACT_TYPE')\n",
    "plt.ylabel('AMT_CREDIT')\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb26301c",
   "metadata": {},
   "source": [
    "The credit amount of loan for cash loans is much higher than revolving loans. Also, the non-defaulters usually borrow more than the defaulters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f6c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the boxplot for AMT_ANNUITY based on REGION_RATING_CLIENT and TARGET\n",
    "sns.boxplot(data=application_train_df, x='REGION_RATING_CLIENT', y='AMT_ANNUITY', hue='TARGET')\n",
    "\n",
    "# Set the labels and title\n",
    "plt.title('AMT_ANNUITY by REGION_RATING_CLIENT and TARGET')\n",
    "plt.xlabel('Region Rating Client')\n",
    "plt.ylabel('Annuity Amount (AMT_ANNUITY)')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8ec5bc",
   "metadata": {},
   "source": [
    "The loan annuity is generally higher in those region rating = 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf36889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the variation in the normalised credit score\n",
    "# from scipy.stats import gaussian_kde\n",
    "\n",
    "# # Replacing inf values with NaN\n",
    "# df_clean = application_train_df.copy()\n",
    "# df_clean.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# # Drop NaN values before plotting\n",
    "# df_clean.dropna(subset=['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3'], inplace=True)\n",
    "\n",
    "# # Plot using Scipy's gaussian_kde for the density plot\n",
    "# plt.figure(figsize=(10, 5))\n",
    "\n",
    "# # Create the KDE for each EXT_SOURCE column\n",
    "# for column in ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']:\n",
    "#     data = df_clean[column].dropna()\n",
    "#     kde = gaussian_kde(data)\n",
    "#     x_vals = np.linspace(data.min(), data.max(), 1000)\n",
    "#     y_vals = kde(x_vals)\n",
    "#     plt.fill(x_vals, y_vals, label=column, alpha=0.5)\n",
    "\n",
    "# # Add legend and show the plot\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e02f33a",
   "metadata": {},
   "source": [
    "From the plot, it is seen that, \n",
    "EXT_SOURCE_2 has a concentrated distribution with a peak of 0.6, suggesting that this score is generally higher and less spread out compared to the other two sources which means it probably has more null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ce609c",
   "metadata": {},
   "source": [
    "## **6. Data Cleaning**\n",
    "###  **a. Data Cleaning on application_train dataset**\n",
    "####  **1. Evaluating columns with missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2600ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering the data that has missing values > 65%\n",
    "def dropna_over65(df):\n",
    "    missing_values = df.isnull().sum().sort_values(ascending=False)\n",
    "    missing_percent = (missing_values / len(df) * 100)\n",
    "\n",
    "    missing_data_over_65 = missing_percent[missing_percent > 65]\n",
    "    print(f'There are: {len(missing_data_over_65)} columns missing data over 65%')\n",
    "    print(missing_data_over_65)\n",
    "\n",
    "#dropping columns that have more than 65% null values\n",
    "    df.drop(columns = missing_data_over_65.index, inplace=True)\n",
    "    print('\\n')\n",
    "    print(f'Shape of the df after removing missing data over 65% : {df.shape}')\n",
    "    \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **b. Factorize all Categorical Columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb5bfa6",
   "metadata": {},
   "source": [
    "Since 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3' seem like important columns, we will first bin the values and factorise these particular columns first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef7c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorize_application(df):\n",
    "    bins = [0, 0.3, 0.6, 0.8, 1.0]\n",
    "    labels = ['Very Poor', 'Average', 'Good', 'Excellent']\n",
    "\n",
    "    # Replacing NaN values with 0\n",
    "    for col in ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']:\n",
    "        df[col] = df[col].fillna(0).astype('float32')  \n",
    "    \n",
    "    # Binning and creating new category columns\n",
    "    for col in ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']:\n",
    "        df[col + '_Category'] = pd.cut(df[col], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    # Dropping original EXT_SOURCE columns\n",
    "    df.drop(columns=['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3'], inplace = True)\n",
    "    \n",
    "    # Factorizing the category columns\n",
    "    for col in ['EXT_SOURCE_1_Category', 'EXT_SOURCE_2_Category', 'EXT_SOURCE_3_Category']:\n",
    "        df[col] = pd.factorize(df[col])[0]\n",
    "\n",
    "    print(f\"Factorized Columns: {['EXT_SOURCE_1_Category', 'EXT_SOURCE_2_Category', 'EXT_SOURCE_3_Category']}\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb5bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "factorize_application(application_train_df)\n",
    "print(application_train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a3dc6",
   "metadata": {},
   "source": [
    "The rest of the categorical columns are factorized here.\n",
    "the labelEncode will detect columns with binary categories are factorize accordingly whereas the OneHotEncoder, will factorize columns with more than two uniq columns. Below is the function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee2acb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorize_cat_cols(df):\n",
    "    \n",
    "    cat_cols = df.select_dtypes(include='object').columns\n",
    "    for col in cat_cols:\n",
    "        df[col] = pd.factorize(df[col])[0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea40d259",
   "metadata": {},
   "source": [
    "#### 2. Evaluating highly-correlated columns\n",
    "Remove highly-correlated numerical variables with greater than 0.8 correlation score using correlation matrix\n",
    "Remove highly-correlated categorical variables and other irrelevant columns using domain knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a9972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric_columns = application_train_df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# correlation_matrix = application_train_df[numeric_columns].corr()\n",
    "\n",
    "# plt.figure(figsize=(20,20))\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths = 0.2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea1233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop highly-correlated numerical variables\n",
    "# Identify pairs of features with correlation above a threshold\n",
    "def redundant_data(df,correlation_matrix):\n",
    "    threshold = 0.8\n",
    "    to_drop = []  # List to store columns to drop\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    correlation_matrix = df[numeric_columns].corr()\n",
    "    \n",
    "# Looping through the correlation matrix to find highly correlated pairs\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                colname = correlation_matrix.columns[i]\n",
    "                if colname not in to_drop:\n",
    "                    to_drop.append(colname)\n",
    "\n",
    "# Drop one column from each highly correlated pair\n",
    "    to_drop = list(set(to_drop) & set(df.columns))\n",
    "\n",
    "    # Drop columns\n",
    "    df.drop(columns=to_drop, inplace=True)\n",
    "    print(f\"Dropped {len(to_drop)} redundant columns\")\n",
    "    print(f\"The columns names that were dropped are :{to_drop}\")\n",
    "    print('\\n')\n",
    "    print(f\"Shape of the dataset after removing multicolinearitly: {df.shape}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2989ff",
   "metadata": {},
   "source": [
    "### ** c. Imputing NA values in the Remaining Columns**\n",
    "For all numeric columns the na values will be replaced with the median of the column \n",
    "for all categorical columns that were factorised, the na values will be replaced with the mode of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d12b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputing_na(df):\n",
    "    for col in df.columns:\n",
    "        if not df[col].dtype == 'number':\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "        else:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    print(\"Imputed all na values\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d9969",
   "metadata": {},
   "source": [
    "### 3. Looking for Columns with Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed05365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing CNT_CHILDREN\n",
    "# We can see some people have 19 children according to the dataset. This is statistically rare.\n",
    "# For any records have have > 6 children, they will be imputed to 6 children.\n",
    "\n",
    "test = application_train_df.copy()\n",
    "test['CNT_CHILDREN'] = np.where(application_train_df['CNT_CHILDREN'] > 6, 6, application_train_df['CNT_CHILDREN'])\n",
    "\n",
    "# implementing the changes back  application_clean_df\n",
    "application_train_df = test.copy()\n",
    "\n",
    "application_train_df['CNT_CHILDREN'].value_counts().sort_index(ascending=True)\n",
    "\n",
    "# CNT Children is much more reasonable now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers_sd(df, threshold=3, exclude_columns=None):\n",
    "    \"\"\"\n",
    "    Caps outliers at ±3 standard deviations for all numeric columns,\n",
    "    except those in exclude_columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    threshold (float): Standard deviation threshold (default is 3).\n",
    "    exclude_columns (list): List of column names to exclude from capping.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with outliers capped for numeric columns only.\n",
    "    \"\"\"\n",
    "    if exclude_columns is None:\n",
    "        exclude_columns = []\n",
    "        \n",
    "    df_capped = df.copy()\n",
    "    # Select only numeric columns\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for column in numeric_columns:\n",
    "        if column in exclude_columns:\n",
    "            continue  # Skip excluded column\n",
    "        \n",
    "        mean = df[column].mean()\n",
    "        std_dev = df[column].std()\n",
    "\n",
    "        lower_bound = mean - threshold * std_dev\n",
    "        upper_bound = mean + threshold * std_dev\n",
    "\n",
    "        df_capped[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "       \n",
    "    return df_capped\n",
    "\n",
    "# Exclude 'CNT_CHILDREN' and apply the function to all numeric columns\n",
    "exclude_cols = ['CNT_CHILDREN', 'TARGET']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea3405",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9113e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling all the functions for data clening\n",
    "application_clean_df = application_train_df.copy()\n",
    "dropna_over65(application_clean_df)\n",
    "\n",
    "factorize_cat_cols(application_clean_df)\n",
    "print(application_clean_df.shape)\n",
    "\n",
    "imputing_na(application_clean_df)\n",
    "print(application_clean_df.shape)\n",
    "\n",
    "redundant_data(application_clean_df,correlation_matrix)\n",
    "print(application_clean_df.shape)\n",
    "\n",
    "df_capped = cap_outliers_sd(application_clean_df, exclude_columns = exclude_cols)\n",
    "print(df_capped.shape)\n",
    "\n",
    "application_clean_df = df_capped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for the updated summaries\n",
    "application_clean_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca7b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for na values\n",
    "application_clean_df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847f9f5",
   "metadata": {},
   "source": [
    "## 7. **Scaling Data**\n",
    "Logistic Regression and most ML models perform better on scaled data because they are sensitive to feature magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f205b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def scale_df(df,target):\n",
    "    X = df.drop(columns=['TARGET'])  # Features\n",
    "    if 'SK_ID_CURR' in X.columns:\n",
    "        X_id = X[['SK_ID_CURR']]  # Retain SK_ID_CURR\n",
    "        X = X.drop(columns=['SK_ID_CURR'])\n",
    "    else:\n",
    "        X_id = None\n",
    "    y = df['TARGET']\n",
    "\n",
    "    # Split Data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 1. Scale the Data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns) \n",
    "    \n",
    "    if X_id is not None:\n",
    "        X_train = pd.concat([X_id.iloc[y_train.index].reset_index(drop=True), X_train], axis=1)\n",
    "        X_test = pd.concat([X_id.iloc[y_test.index].reset_index(drop=True), X_test], axis=1)\n",
    "        \n",
    "    print(\"Data Scaling Done\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ce0e8",
   "metadata": {},
   "source": [
    "## 8. **2. Handle Class Imbalance**\n",
    "Imbalance skews the model towards the majority class, making it harder to predict defaults (1s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9a8b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Undersample the Majority Class\n",
    "def under_sample(X_train, y_train):\n",
    "    if 'SK_ID_CURR' in X_train.columns:\n",
    "        X_id = X_train[['SK_ID_CURR']]\n",
    "        X_train = X_train.drop(columns=['SK_ID_CURR'])\n",
    "    else:\n",
    "        X_id = None\n",
    "        \n",
    "    undersample = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n",
    "    X_train, y_train = undersample.fit_resample(X_train, y_train)\n",
    "    \n",
    "    if X_id is not None:\n",
    "        X_train = pd.concat([X_id.iloc[y_train.index].reset_index(drop=True), X_train], axis=1)\n",
    "        \n",
    "    print(\"Class Imbalance Fixed\")\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07719ee",
   "metadata": {},
   "source": [
    "## 9.**Feature Selection (Top Predictors)**\n",
    "Helps reduce dimensionality and removes noisy features before final model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4966422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def imp_features(X_train, X_test, y_train, num_features=10):\n",
    "    # Ensure SK_ID_CURR is kept separately\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Get feature importance scores\n",
    "    feature_importance = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "\n",
    "    # Select top 'num_features' based on importance\n",
    "    selected_cols = feature_importance.nlargest(num_features).index\n",
    "\n",
    "    # Restore SK_ID_CURR if it was present\n",
    "    if 'SK_ID_CURR' in X_train.columns:\n",
    "        X_train_id = X_train[['SK_ID_CURR']]\n",
    "        X_test_id = X_test[['SK_ID_CURR']]\n",
    "        \n",
    "        X_train = pd.concat([X_train_id, X_train[selected_cols]], axis=1)\n",
    "        X_test = pd.concat([X_test_id, X_test[selected_cols]], axis=1)\n",
    "    else:\n",
    "        X_train = X_train[selected_cols]\n",
    "        X_test = X_test[selected_cols]\n",
    "    print(selected_cols)\n",
    "\n",
    "    return X_train, X_test, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8384738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_final_df = application_clean_df.copy() \n",
    "target = 'TARGET'  # Your Target Column Name\n",
    "\n",
    "# 1. Scale Data\n",
    "X_train, X_test, y_train, y_test = scale_df(application_final_df,target)\n",
    "\n",
    "# 2. Fix Class Imbalance\n",
    "X_train, y_train = under_sample(X_train, y_train)\n",
    "\n",
    "# 3. Feature Selection\n",
    "X_train, X_test, y_train = imp_features(X_train, X_test, y_train)\n",
    "\n",
    "print(f\"Final X_train Shape: {X_train.shape}\")\n",
    "print(f\"Final X_test Shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796987a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Training The model Using various methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970e8a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "### a. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a253f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Train Logistic Regression model with top 10 features\n",
    "model_log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model_log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate model performance \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Logistic Regression Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b4bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###c. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c04df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALI - adding a KNN model\n",
    "## **** ALI: TO DO *** ADD GRIDSEARCHCV TOO\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize and train KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Evaluate the KNN model\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"KNN Accuracy: {accuracy_knn:.4f}\")\n",
    "print(\"KNN Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_knn)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"KNN Confusion Matrix\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a9fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### b. Random Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RFC = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "model_RFC.fit(X_train, y_train)\n",
    "\n",
    "random_predictions = np.random.choice([0, 1], size=y_test.shape[0], p=[0.7, 0.3])\n",
    "\n",
    "# Calculate accuracy of the random classifier\n",
    "random_accuracy = accuracy_score(y_test, random_predictions)\n",
    "print(f\"Random Classifier Accuracy: {random_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e09c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Creating an ROC curve\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# fpr, tpr, thresholds = roc_curve(y_train, y_train_preds_proba)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# plt.plot(fpr, tpr)\n",
    "# plt.plot([0,1], [0,1])\n",
    "# plt.xlabel('False positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC curve')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3fdcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exploring Bureau Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cbb759",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merging other Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcbb696",
   "metadata": {},
   "source": [
    "The bureau_df has a column named CREDIT_ACTIVE, which has records of the applicants credit history. Every applicant seems to have atleast two records of credits either active or closed. We have tried to convert these rows of data into one and merge it with the main dataset. CREDIT_ACTIVE seems to be an important column for an applicant to default, so we are merging the dataset to improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aa8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bureau_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4710e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_clean_df = bureau_df.copy()\n",
    "dropna_over65(bureau_clean_df)\n",
    "\n",
    "factorize_cat_cols(bureau_clean_df)\n",
    "print(bureau_clean_df.shape)\n",
    "\n",
    "imputing_na(bureau_clean_df)\n",
    "print(bureau_clean_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "redundant_data(bureau_clean_df,correlation_matrix)\n",
    "print(bureau_clean_df.shape)\n",
    "\n",
    "df_capped = cap_outliers_sd(bureau_clean_df, exclude_columns = exclude_cols)\n",
    "print(df_capped.shape)\n",
    "\n",
    "bureau_clean_df = df_capped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399fbe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting multiple records per ID into one based on the condition\n",
    "bureau_summarised_df = bureau_clean_df.groupby(\"SK_ID_CURR\")[\"CREDIT_ACTIVE\"].apply(lambda x: \"Active\" if \"Active\" in x.values else \"Closed\").reset_index()\n",
    "\n",
    "# Display the result\n",
    "print(bureau_summarised_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80061843",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bureau_summarised_df[bureau_summarised_df.duplicated(subset=[\"SK_ID_CURR\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_bureau_df = pd.merge(bureau_summarised_df,bureau_clean_df, on = \"SK_ID_CURR\", how = \"inner\")\n",
    "merged_bureau_df = merged_bureau_df.drop_duplicates(subset=[\"SK_ID_CURR\"])\n",
    "print(merged_bureau_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb68e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cade173",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = X_train.columns[X_train.columns.duplicated()]\n",
    "print(\"Duplicate column names:\", duplicates)\n",
    "# If there are duplicates (e.g., two columns with the same name like 'SK_ID_CURR')\n",
    "if len(duplicates) > 0:\n",
    "    # Drop the second 'SK_ID_CURR' column\n",
    "    X_train = X_train.loc[:, ~X_train.columns.duplicated()]\n",
    "\n",
    "# Print the updated shape of X_train to confirm the change\n",
    "print(X_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb08ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.merge(merged_bureau_df, X_train, on = \"SK_ID_CURR\", how = \"inner\")\n",
    "print(new_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_df = imp_features(new_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
